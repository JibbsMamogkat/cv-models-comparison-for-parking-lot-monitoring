# A Statistical Comparison of Deep Learning Models for Multiclass Parking Vacancy Counting

This is the official repository for our CPE110-1 research project. This study conducts a rigorous, statistically significant comparison of four deep learning architectures to determine the most accurate model for a **multiclass parking vacancy counting task**, based on faculty-provided guidelines.

**Group Members:**
* Mohammad Jameel Jibreel N. Mamogkat
* Duff S. Bastasa
* Jake Lloyd A. Ganoy

---

## Table of Contents
1.  [Revised Project Goal](#revised-project-goal)
2.  [Systems Engineering Overview](#systems-engineering-overview)
3.  [Folder Structure Explained](#folder-structure-explained)
4.  [Project Workflow](#project-workflow)
5.  [Setup and Dependencies](#setup-and-dependencies)

---

## Revised Project Goal

The primary objective is to determine which of four deep learning models **(MobileNetV2, EfficientNetB0, YOLOv8, and Mask R-CNN)** provides the most statistically accurate performance for counting parking lot availability. This involves a **multiclass classification** task, where each parking space must be identified as containing a `car`, a `motorcycle`, or being `vacant`.

The final evaluation will focus on a rigorous analysis of model accuracy, culminating in an **ANOVA test** to validate the statistical significance of the performance differences between the models.

---

## Systems Engineering Overview

The project is architected as a **Comparative Analysis Pipeline**. The system is designed to ingest and process data, train four parallel models, and run them through a rigorous evaluation engine to produce statistically validated results. This modular structure is reflected in our `scripts/` directory.

#### Subsystem 1: Data Preprocessing & Harmonization
* **Function:** Ingests raw academic datasets (PKLot, CNRPark+EXT). Executes a Python script to parse, clean, and convert the data into standardized formats suitable for a 3-class (`car`, `motorcycle`, `vacant`) training task.
* **Responsible Script:** `scripts/1_harmonize_data.py`

#### Subsystem 2: Parallel Training Module
* **Function:** Trains all four model architectures on the harmonized, multiclass dataset. Each model has its own dedicated training script.
* **Responsible Scripts:** `scripts/2a_train_yolo.py`, `scripts/2b_train_maskrcnn.py`, etc.

#### Subsystem 3: Quantitative Evaluation Engine
* **Function:** Evaluates the trained models against two distinct test sets: (1) The standard academic test set and (2) Our custom **"Real-World Generalization Set"** of 50 images from Davao City. The engine's primary outputs are a **3x3 multiclass confusion matrix** and an overall accuracy score for each model.
* **Responsible Script:** `scripts/3_evaluate_models.py`

#### Subsystem 4: Statistical Analysis Module
* **Function:** Ingests the final accuracy scores from the evaluation engine and performs an **ANOVA test** to determine if the performance differences between models are statistically significant.
* **Responsible Script:** `scripts/4_run_anova.py`

---

## Folder Structure Explained

* `üìÅ data/`: Contains all project data.
    * `üìÅ raw/`: **(Git Ignored)** Place the original, untouched downloaded academic datasets (`PKLot`, `CNRPark+EXT`) here.
    * `üìÅ processed/`: **(Git Ignored)** The output of the harmonization script will go here. This is the clean data our training scripts will use.
    * `üìÅ real_world_test_set/`: Contains our 50 custom images taken in Davao City and their corresponding label files.

* `üìÅ scripts/`: Contains all executable Python code, with each script corresponding to a subsystem in our pipeline.

* `üìÅ models/`: **(Git Ignored)** Final trained model files (`.pt`, `.pth`, etc.) will be saved here after training.

* `üìÅ results/`: Contains all outputs from our experiments.
    * `üìÅ charts/`: Confusion matrices and other charts generated by the evaluation script will be saved here.
    * `üìú evaluation_scores.csv`: The master CSV file with accuracy scores, serving as the input for our final statistical analysis.

* `üìú .gitignore`: A critical file that tells Git to ignore large directories like `data/` and `models/`. This keeps our repository lightweight.

* `üìú requirements.txt`: A list of all the Python libraries required to run this project, ensuring a consistent environment for all group members.

---

## Project Workflow

To run the entire pipeline from start to finish, the scripts should be executed in the following order:

1.  **Data Collection:** Manually capture and label the 50 images for the `data/real_world_test_set/`.
2.  **Setup the Environment:** Run `pip install -r requirements.txt` to install all necessary libraries.
3.  **Harmonize Data:** Run `python scripts/1_harmonize_data.py`.
4.  **Train Models:** Run all four training scripts (e.g., `python scripts/2a_train_yolo.py`). This is the most time-intensive step.
5.  **Evaluate Models:** Once models are trained, run `python scripts/3_evaluate_models.py` to generate accuracy scores and confusion matrices.
6.  **Run Statistical Analysis:** Finally, run `python scripts/4_run_anova.py` on the `results/evaluation_scores.csv` to obtain the final answer to our research question.

---

## Setup and Dependencies

To set up the project environment on a new machine, follow these steps:

1.  **Clone the Repository:**
    ```bash
    git clone [https://github.com/JibbsMamogkat/cv-models-comparison.git](https://github.com/JibbsMamogkat/cv-models-comparison.git)
    cd cv-models-comparison
    ```
2.  **Create and Activate a Virtual Environment:**
    ```bash
    python -m venv venv
    # On Windows (Git Bash):
    source venv/Scripts/activate
    ```
3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
4.  **Download Academic Data:** Manually download the PKLot and CNRPark+EXT datasets and place the `.zip` files in the `data/raw/` directory.

The project is now ready for development and execution.